{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4f2212",
   "metadata": {},
   "source": [
    "# Webscraping more headlines for my database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4dc3488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901d635e",
   "metadata": {},
   "source": [
    "## Getting the headlines using Beautifulsoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f7ffdf",
   "metadata": {},
   "source": [
    "### BBC news webscrapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f86a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICC prosecutor seeks arrest warrants for Netanyahu and Hamas leaders\n",
      "Iran's President Ebrahim Raisi killed in helicopter crash\n",
      "Ship that hit Baltimore bridge on the move again\n",
      "ChatGPT to lose voice over Johansson similarity\n",
      "How should countries deal with falling birth rates?\n",
      "Iran's President Ebrahim Raisi killed in helicopter crash\n",
      "What next for Iran after President Raisi's death?\n",
      "Assange wins right to challenge US extradition\n",
      "Cohen admits to stealing in tense cross-examination\n",
      "Ship that hit Baltimore bridge on the move again\n",
      "'Stop threatening us', Taiwan's new president tells China\n",
      "Victims say 'now the world knows' about UK infected blood cover-up\n",
      "Lil Nas X: I felt like an imposter on tour\n",
      "Three Americans detained in alleged DR Congo coup attempt\n",
      "Retail mogul Bruce Nordstrom dies, aged 90\n",
      "Iranian president killed\n",
      "Ebrahim Raisi: What we know about deadly Iran helicopter crash\n",
      "Iran declares five days of mourning for president\n",
      "Watch: Iran state TV announces Raisi's death\n",
      "Who is in charge of Iran?\n",
      "Bodies recovered from Iran helicopter crash site\n",
      "Features & analysis\n",
      "Did prosecutors make a strong case in Trump trial?\n",
      "Newscast\n",
      "People want 'dumbphones'. Will big tech deliver?\n",
      "Watch: How Iran state TV broke news of president's death\n",
      "Three-quarters of Gaza marked as IDF evacuation zones, BBC finds\n",
      "Newscast\n",
      "Should London become a 'sponge city'?\n",
      "War and popularity keep Zelensky in power despite term expiring\n",
      "People want 'dumbphones'. Will big tech deliver?\n",
      "Most watched\n",
      "Baby Reindeer 'a big problem' for Netflix and Gadd - Morgan\n",
      "Trump attends son Barron's graduation in Florida\n",
      "Watch: ICC makes Israel and Hamas announcement\n",
      "Watch: How Iran state TV broke news of president's death\n",
      "Watch: Iran state TV announces Raisi's death\n",
      "Also in news\n",
      "Deep green sea? The oceans are changing colour\n",
      "Dublin-New York portal reopens after 'inappropriate behaviour'\n",
      "Metro disabled access 'shame' for Paralympic Paris\n",
      "Guardiola turns Man City into PL's great untouchables\n",
      "Scheffler 'fairly tired' after 'hectic' PGA weekend\n",
      "Dublin-New York portal reopens after 'inappropriate behaviour'\n",
      "Diddy apologises after video shows attack on ex-girlfriend\n",
      "Delhi sizzles as temperatures cross 45C\n",
      "Metro disabled access 'shame' for Paralympic Paris\n",
      "Most read\n",
      "Ship that hit Baltimore bridge on the move again\n",
      "More Northern Lights soon as Sun storms strengthen\n",
      "'We're in our 20s but live in the 1940s'\n",
      "ChatGPT to lose voice over Johansson similarity\n",
      "ICC prosecutor seeks arrest of Israeli and Hamas leaders\n",
      "What next for Iran after President Raisi's death?\n",
      "'Stop threatening us', Taiwan's new president tells China\n",
      "War and popularity keep Zelensky in power despite term expiring\n",
      "How should countries deal with falling birth rates?\n",
      "Assange wins right to challenge US extradition\n",
      "Sport\n",
      "Liverpool job 'difficult to ignore' - Slot\n",
      "'The Valhalla nail-biter that had everything'\n",
      "Archer will bring 'fear factor' on England return\n",
      "Guardiola turns Man City into PL's great untouchables\n",
      "Timberwolves knock holders Nuggets out of play-offs\n",
      "'The Valhalla nail-biter that had everything'\n",
      "Phil McNulty's end-of-season Premier League report\n",
      "Archer will bring 'fear factor' on England return\n"
     ]
    }
   ],
   "source": [
    "# URL of the BBC News homepage\n",
    "url = \"https://www.bbc.com/news\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all <h2> tags\n",
    "    h2_tags = soup.find_all('h2')\n",
    "    \n",
    "    # Extract and print the titles from the <h2> tags\n",
    "    for tag in h2_tags:\n",
    "        print(tag.get_text(strip=True))\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aceef0",
   "metadata": {},
   "source": [
    "## Returning the result as Json\n",
    "\n",
    "GPT prompt: \n",
    "\n",
    "\"now, edit the json file to transform it in a dictionary like this one: \n",
    "\n",
    "{\n",
    "    \"label\": 1,\n",
    "    \"text\": \"Simone Biles wins Core Hydration Classic on road to Paris 2024 Olympics\"\n",
    "    },\n",
    "\n",
    "\n",
    "leave the label empty, but add to the \"text\"  for  each headline in the json file you gave me. I will fill the \"label\" myself.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "719cf952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles saved successfully to bbc_news_titles.json\n"
     ]
    }
   ],
   "source": [
    "def get_bbc_news_titles(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all <h2> tags\n",
    "        h2_tags = soup.find_all('h2')\n",
    "        \n",
    "        # Extract titles from the <h2> tags and transform them into the desired dictionary format\n",
    "        titles = [{\"label\": None, \"text\": tag.get_text(strip=True)} for tag in h2_tags]\n",
    "        \n",
    "        # Return the result as a list of dictionaries\n",
    "        return titles\n",
    "    else:\n",
    "        # If the request failed, return an error message as a dictionary\n",
    "        return {\"error\": f\"Failed to retrieve the page. Status code: {response.status_code}\"}\n",
    "\n",
    "# URL of the BBC News homepage\n",
    "url = \"https://www.bbc.com/news\"\n",
    "\n",
    "# Get the titles as a list of dictionaries\n",
    "titles = get_bbc_news_titles(url)\n",
    "\n",
    "# Check if the result is an error message\n",
    "if \"error\" in titles:\n",
    "    print(titles[\"error\"])\n",
    "else:\n",
    "    # Save the result to a JSON file\n",
    "    output_file = \"bbc_news_titles.json\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(titles, f, indent=4)\n",
    "    \n",
    "    print(f\"Titles saved successfully to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9e64734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects in the BBC JSON file: 44\n",
      "Number of labels with value 0: 40\n",
      "Number of labels with value 1: 0\n"
     ]
    }
   ],
   "source": [
    "# Open the JSON file and load the data\n",
    "with open(\"bbc_news_titles.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Check the number of objects\n",
    "num_objects_bbc = len(data)\n",
    "\n",
    "print(\"Number of objects in the BBC JSON file:\", num_objects_bbc)\n",
    "\n",
    "# Open the JSON file and load the data\n",
    "with open(\"bbc_news_titles.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Count the occurrences of labels\n",
    "label_counts = {\"0\": 0, \"1\": 0}\n",
    "for item in data:\n",
    "    if item[\"label\"] == 0:\n",
    "        label_counts[\"0\"] += 1\n",
    "    elif item[\"label\"] == 1:\n",
    "        label_counts[\"1\"] += 1\n",
    "\n",
    "# Print the label counts\n",
    "print(\"Number of labels with value 0:\", label_counts[\"0\"])\n",
    "print(\"Number of labels with value 1:\", label_counts[\"1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab2e978",
   "metadata": {},
   "source": [
    "### The Guardian news webscrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6b5c04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aria-labels saved successfully to the_guardian.json\n"
     ]
    }
   ],
   "source": [
    "def get_guardian_aria_labels(url):\n",
    "    # List of unwanted \"aria-label\" values\n",
    "    unwanted_labels = [\n",
    "        \"Toggle main menu\",\n",
    "        \"Toggle News\",\n",
    "        \"Toggle Opinion\",\n",
    "        \"Toggle Sport\",\n",
    "        \"Toggle Culture\",\n",
    "        \"Toggle Lifestyle\"\n",
    "    ]\n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all elements with \"aria-label\" attribute\n",
    "        elements_with_aria_label = soup.find_all(attrs={\"aria-label\": True})\n",
    "        \n",
    "        # Extract the content of \"aria-label\" attribute from each element\n",
    "        aria_labels = [\n",
    "            {\"label\": None, \"text\": element[\"aria-label\"]}\n",
    "            for element in elements_with_aria_label\n",
    "            if element[\"aria-label\"] not in unwanted_labels\n",
    "        ]\n",
    "        \n",
    "        # Return the result as a JSON object\n",
    "        return json.dumps(aria_labels, indent=4)\n",
    "    else:\n",
    "        # If the request failed, return an error message as a JSON object\n",
    "        return json.dumps({\"error\": f\"Failed to retrieve the page. Status code: {response.status_code}\"}, indent=4)\n",
    "\n",
    "# URL of the Guardian homepage\n",
    "url = \"https://www.theguardian.com/international\"\n",
    "\n",
    "# Get the \"aria-label\" content as a JSON object\n",
    "aria_labels_json = get_guardian_aria_labels(url)\n",
    "\n",
    "# Save the result to a JSON file\n",
    "output_file = \"the_guardian.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(aria_labels_json)\n",
    "\n",
    "# Print a message indicating that the data has been saved\n",
    "print(f\"Aria-labels saved successfully to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ec921431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects in the GUARDIAN JSON file: 114\n",
      "Number of labels with value 0: 0\n",
      "Number of labels with value 1: 0\n"
     ]
    }
   ],
   "source": [
    "# Open the JSON file and load the data\n",
    "with open(\"the_guardian.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Check the number of objects\n",
    "num_objects_guardian = len(data)\n",
    "\n",
    "print(\"Number of objects in the GUARDIAN JSON file:\", num_objects_guardian)\n",
    "\n",
    "# Open the JSON file and load the data\n",
    "with open(\"the_guardian.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Count the occurrences of labels\n",
    "label_counts = {\"0\": 0, \"1\": 0}\n",
    "for item in data:\n",
    "    if item[\"label\"] == 0:\n",
    "        label_counts[\"0\"] += 1\n",
    "    elif item[\"label\"] == 1:\n",
    "        label_counts[\"1\"] += 1\n",
    "\n",
    "# Print the label counts\n",
    "print(\"Number of labels with value 0:\", label_counts[\"0\"])\n",
    "print(\"Number of labels with value 1:\", label_counts[\"1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "221ddb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content saved successfully to good_news.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def scrape_good_news(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all <strong> tags\n",
    "        strong_tags = soup.find_all('strong')\n",
    "        \n",
    "        # Extract text from the <strong> tags and format into dictionaries\n",
    "        data = [{\"label\": 1, \"text\": tag.get_text(strip=True)} for tag in strong_tags]\n",
    "        \n",
    "        # Return the result as a JSON object\n",
    "        return json.dumps(data, indent=4)\n",
    "    else:\n",
    "        # If the request failed, return an error message as a JSON object\n",
    "        return json.dumps({\"error\": f\"Failed to retrieve the page. Status code: {response.status_code}\"}, indent=4)\n",
    "\n",
    "# URL of the webpage\n",
    "url = \"http://www.goodnewsagency.org/m/issue.php?number=324&lang=en\"\n",
    "\n",
    "# Get the content as a JSON object\n",
    "content_json = scrape_good_news(url)\n",
    "\n",
    "# Save the result to a JSON file\n",
    "output_file = \"good_news.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(content_json)\n",
    "\n",
    "# Print a message indicating that the data has been saved\n",
    "print(f\"Content saved successfully to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e245f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects in the GOOD NEWS JSON file: 48\n",
      "Number of labels with value 0: 0\n",
      "Number of labels with value 1: 48\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open the JSON file and load the data\n",
    "with open(\"good_news.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Check the number of objects\n",
    "num_objects_good_news = len(data)\n",
    "\n",
    "print(\"Number of objects in the GOOD NEWS JSON file:\", num_objects_good_news)\n",
    "\n",
    "# Open the JSON file and load the data\n",
    "with open(\"good_news.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Count the occurrences of labels\n",
    "label_counts = {\"0\": 0, \"1\": 0}\n",
    "for item in data:\n",
    "    if item[\"label\"] == 0:\n",
    "        label_counts[\"0\"] += 1\n",
    "    elif item[\"label\"] == 1:\n",
    "        label_counts[\"1\"] += 1\n",
    "\n",
    "# Print the label counts\n",
    "print(\"Number of labels with value 0:\", label_counts[\"0\"])\n",
    "print(\"Number of labels with value 1:\", label_counts[\"1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f8828f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles extracted and saved to 'good_news_network_titles.json'.\n"
     ]
    }
   ],
   "source": [
    "def extract_titles(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all <a> tags with the specified attributes\n",
    "        a_tags = soup.find_all('a', attrs={'rel': 'bookmark'})\n",
    "        \n",
    "        # Extract titles from the title attribute of each <a> tag\n",
    "        titles = [{\"label\": 1, \"text\":tag.get('title', '')} for tag in a_tags]\n",
    "        \n",
    "        # Return the result as a JSON object\n",
    "        return json.dumps(titles, indent=4)\n",
    "    else:\n",
    "        # If the request failed, return an error message as a JSON object\n",
    "        return json.dumps({\"error\": f\"Failed to retrieve the page. Status code: {response.status_code}\"}, indent=4)\n",
    "\n",
    "# URL of the webpage\n",
    "url = \"https://www.goodnewsnetwork.org/category/news/world/\"\n",
    "\n",
    "# Get the titles as a JSON object\n",
    "titles_json = extract_titles(url)\n",
    "\n",
    "# Save the titles to a JSON file\n",
    "with open(\"good_news_network_titles.json\", \"w\") as file:\n",
    "    file.write(titles_json)\n",
    "\n",
    "# Print a success message\n",
    "print(\"Titles extracted and saved to 'good_news_network_titles.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a7cded0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects in the good_news_network_titles.json file: 33\n",
      "Number of labels with value 0: 0\n",
      "Number of labels with value 1: 33\n"
     ]
    }
   ],
   "source": [
    "# Open the JSON file and load the data\n",
    "with open(\"good_news_network_titles.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Check the number of objects\n",
    "num_objects_goodnews_network = len(data)\n",
    "\n",
    "print(\"Number of objects in the good_news_network_titles.json file:\", num_objects_goodnews_network)\n",
    "\n",
    "# Open the JSON file and load the data\n",
    "with open(\"good_news_network_titles.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Count the occurrences of labels\n",
    "label_counts = {\"0\": 0, \"1\": 0}\n",
    "for item in data:\n",
    "    if item[\"label\"] == 0:\n",
    "        label_counts[\"0\"] += 1\n",
    "    elif item[\"label\"] == 1:\n",
    "        label_counts[\"1\"] += 1\n",
    "\n",
    "# Print the label counts\n",
    "print(\"Number of labels with value 0:\", label_counts[\"0\"])\n",
    "print(\"Number of labels with value 1:\", label_counts[\"1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6cee967f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles extracted and saved to 'good_news_inspiring.json'.\n"
     ]
    }
   ],
   "source": [
    "def extract_titles(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all <a> tags with the specified attributes\n",
    "        a_tags = soup.find_all('a', attrs={'rel': 'bookmark'})\n",
    "        \n",
    "        # Extract titles from the title attribute of each <a> tag\n",
    "        titles = [{\"label\": 1, \"text\":tag.get('title', '')} for tag in a_tags]\n",
    "        \n",
    "        # Return the result as a JSON object\n",
    "        return json.dumps(titles, indent=4)\n",
    "    else:\n",
    "        # If the request failed, return an error message as a JSON object\n",
    "        return json.dumps({\"error\": f\"Failed to retrieve the page. Status code: {response.status_code}\"}, indent=4)\n",
    "\n",
    "# URL of the webpage\n",
    "url = \"https://www.goodnewsnetwork.org/category/news/inspiring/\"\n",
    "\n",
    "# Get the titles as a JSON object\n",
    "titles_json = extract_titles(url)\n",
    "\n",
    "# Save the titles to a JSON file\n",
    "with open(\"good_news_inspiring.json\", \"w\") as file:\n",
    "    file.write(titles_json)\n",
    "\n",
    "# Print a success message\n",
    "print(\"Titles extracted and saved to 'good_news_inspiring.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "94bdc026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects in the good_news_inspiring.json file: 30\n",
      "Number of labels with value 0: 0\n",
      "Number of labels with value 1: 30\n"
     ]
    }
   ],
   "source": [
    "# Open the JSON file and load the data\n",
    "with open(\"good_news_inspiring.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Check the number of objects\n",
    "num_objects_goodnews_inspiring = len(data)\n",
    "\n",
    "print(\"Number of objects in the good_news_inspiring.json file:\", num_objects_goodnews_inspiring)\n",
    "\n",
    "# Open the JSON file and load the data\n",
    "with open(\"good_news_inspiring.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Count the occurrences of labels\n",
    "label_counts = {\"0\": 0, \"1\": 0}\n",
    "for item in data:\n",
    "    if item[\"label\"] == 0:\n",
    "        label_counts[\"0\"] += 1\n",
    "    elif item[\"label\"] == 1:\n",
    "        label_counts[\"1\"] += 1\n",
    "\n",
    "# Print the label counts\n",
    "print(\"Number of labels with value 0:\", label_counts[\"0\"])\n",
    "print(\"Number of labels with value 1:\", label_counts[\"1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e805dae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total positive news:  121\n",
      "total negative news:  164\n"
     ]
    }
   ],
   "source": [
    "total_positive = num_objects_goodnews_network + num_objects_good_news + 13 + num_objects_goodnews_inspiring\n",
    "print(\"total positive news: \", total_positive)\n",
    "\n",
    "total_negative = num_objects_bbc + num_objects_guardian + 6\n",
    "print(\"total negative news: \", total_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4d1ee336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels with value 0: 6\n",
      "Number of labels with value 1: 13\n"
     ]
    }
   ],
   "source": [
    "# Open the JSON file and load the data\n",
    "with open(\"dataset.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# # Check the number of objects\n",
    "# num_objects_goodnews_network = len(data)\n",
    "\n",
    "# print(\"Number of objects in the good_news_network_titles.json file:\", num_objects_goodnews_network)\n",
    "\n",
    "# Open the JSON file and load the data\n",
    "with open(\"dataset.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Count the occurrences of labels\n",
    "label_counts = {\"0\": 0, \"1\": 0}\n",
    "for item in data:\n",
    "    if item[\"label\"] == 0:\n",
    "        label_counts[\"0\"] += 1\n",
    "    elif item[\"label\"] == 1:\n",
    "        label_counts[\"1\"] += 1\n",
    "\n",
    "# Print the label counts\n",
    "print(\"Number of labels with value 0:\", label_counts[\"0\"])\n",
    "print(\"Number of labels with value 1:\", label_counts[\"1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a3fdbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
