{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c72f77",
   "metadata": {},
   "source": [
    "# Webscraping more headlines for my database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad15b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd2fe7",
   "metadata": {},
   "source": [
    "## Getting the headlines using Beautifulsoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d55a43",
   "metadata": {},
   "source": [
    "## Getting some negative news:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a8a462",
   "metadata": {},
   "source": [
    "###  Webscrapping: BBC news \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b57e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Netanyahu denounces bid to arrest him over Gaza war\n",
      "Iran declares five days of mourning for president\n",
      "Decades after training, 90-year-old finally goes to space\n",
      "ChatGPT to lose voice over Johansson similarity\n",
      "Assange wins right to challenge US extradition\n",
      "Iran declares five days of mourning for president\n",
      "What next for Iran after President Raisi's death?\n",
      "Drama at Trump trial as judge reprimands witness and clears court\n",
      "Ship that hit Baltimore bridge on the move again\n",
      "Decades after training, 90-year-old finally goes to space\n",
      "Electric pulses may ease paralysis after broken neck\n",
      "Scottie Scheffler court date pushed back after arrest\n",
      "'Stop threatening us', Taiwan's new president tells China\n",
      "Three Americans detained in alleged DR Congo coup attempt\n",
      "How should countries deal with falling birth rates?\n",
      "Iranian president killed\n",
      "Ebrahim Raisi: What we know about deadly Iran helicopter crash\n",
      "How Iranians reacted to president's helicopter crash\n",
      "Who is in charge of Iran?\n",
      "President Ebrahim Raisi's mixed legacy in Iran\n",
      "Bodies recovered from Iran helicopter crash site\n",
      "Features & analysis\n",
      "Did prosecutors make a strong case in Trump trial?\n",
      "Newscast\n",
      "People want 'dumbphones'. Will big tech deliver?\n",
      "How Iranians reacted to president's helicopter crash\n",
      "Three-quarters of Gaza marked as IDF evacuation zones, BBC finds\n",
      "Newscast\n",
      "Should London become a 'sponge city'?\n",
      "War and popularity keep Zelensky in power despite term expiring\n",
      "People want 'dumbphones'. Will big tech deliver?\n",
      "Most watched\n",
      "Decades after training, 90-year-old finally goes to space\n",
      "Trump attends son Barron's graduation in Florida\n",
      "Baby Reindeer 'a big problem' for Netflix and Gadd - Morgan\n",
      "Tornadoes and dust storms hit Oklahoma and Kansas\n",
      "Bodies recovered from Iran helicopter crash site\n",
      "Also in news\n",
      "Deep green sea? The oceans are changing colour\n",
      "Tornadoes and dust storms hit Oklahoma and Kansas\n",
      "Lil Nas X: I felt like an imposter on tour\n",
      "Retail mogul Bruce Nordstrom dies, aged 90\n",
      "Guardiola turns Man City into PL's great untouchables\n",
      "Tornadoes and dust storms hit Oklahoma and Kansas\n",
      "Delhi sizzles as temperatures cross 45C\n",
      "Dublin-New York portal reopens after 'inappropriate behaviour'\n",
      "Lil Nas X: I felt like an imposter on tour\n",
      "Most read\n",
      "Netanyahu denounces bid to arrest him over Gaza war\n",
      "More Northern Lights soon as Sun storms strengthen\n",
      "'We're in our 20s but live in the 1940s'\n",
      "Iran declares five days of mourning for president\n",
      "Ship that hit Baltimore bridge on the move again\n",
      "Ebrahim Raisi: What we know about deadly Iran helicopter crash\n",
      "War and popularity keep Zelensky in power despite term expiring\n",
      "ChatGPT to lose voice over Johansson similarity\n",
      "How should countries deal with falling birth rates?\n",
      "What next for Iran after President Raisi's death?\n",
      "Sport\n",
      "Liverpool job 'difficult to ignore' - Slot\n",
      "'The Valhalla nail-biter that had everything'\n",
      "Archer will bring 'fear factor' on England return\n",
      "Guardiola turns Man City into PL's great untouchables\n",
      "Timberwolves knock holders Nuggets out of play-offs\n",
      "'The Valhalla nail-biter that had everything'\n",
      "Phil McNulty's end-of-season Premier League report\n",
      "Archer will bring 'fear factor' on England return\n"
     ]
    }
   ],
   "source": [
    "# URL of the BBC News homepage\n",
    "url = \"https://www.bbc.com/news\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all <h2> tags\n",
    "    h2_tags = soup.find_all('h2')\n",
    "    \n",
    "    # Extract and print the titles from the <h2> tags\n",
    "    for tag in h2_tags:\n",
    "        print(tag.get_text(strip=True))\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc41d8",
   "metadata": {},
   "source": [
    "## Returning the result as Json\n",
    "\n",
    "GPT prompt: \n",
    "\n",
    "\"now, edit the json file to transform it in a dictionary like this one: \n",
    "\n",
    "{\n",
    "    \"label\": 1,\n",
    "    \"text\": \"Simone Biles wins Core Hydration Classic on road to Paris 2024 Olympics\"\n",
    "    },\n",
    "\n",
    "\n",
    "leave the label empty, but add to the \"text\"  for  each headline in the json file you gave me. I will fill the \"label\" myself.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70508680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles saved successfully to bbc_news_titles.json\n"
     ]
    }
   ],
   "source": [
    "def get_bbc_news_titles(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all <h2> tags\n",
    "        h2_tags = soup.find_all('h2')\n",
    "        \n",
    "        # Extract titles from the <h2> tags and transform them into the desired dictionary format\n",
    "        titles = [{\"label\": 0, \"text\": tag.get_text(strip=True)} for tag in h2_tags]\n",
    "        \n",
    "        # Return the result as a list of dictionaries\n",
    "        return titles\n",
    "    else:\n",
    "        # If the request failed, return an error message as a dictionary\n",
    "        return {\"error\": f\"Failed to retrieve the page. Status code: {response.status_code}\"}\n",
    "\n",
    "# URL of the BBC News homepage\n",
    "url = \"https://www.bbc.com/news\"\n",
    "\n",
    "# Get the titles as a list of dictionaries\n",
    "titles = get_bbc_news_titles(url)\n",
    "\n",
    "# Check if the result is an error message\n",
    "if \"error\" in titles:\n",
    "    print(titles[\"error\"])\n",
    "else:\n",
    "    # Save the result to a JSON file\n",
    "    output_file = \"bbc_news_titles.json\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(titles, f, indent=4)\n",
    "    \n",
    "    print(f\"Titles saved successfully to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d370300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects in the BBC JSON file: 67\n",
      "Number of labels with value 0: 67\n",
      "Number of labels with value 1: 0\n"
     ]
    }
   ],
   "source": [
    "# Open the JSON file and load the data\n",
    "with open(\"bbc_news_titles.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Check the number of objects\n",
    "num_objects_bbc = len(data)\n",
    "\n",
    "print(\"Number of objects in the BBC JSON file:\", num_objects_bbc)\n",
    "\n",
    "# Open the JSON file and load the data\n",
    "with open(\"bbc_news_titles.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Count the occurrences of labels\n",
    "label_counts = {\"0\": 0, \"1\": 0}\n",
    "for item in data:\n",
    "    if item[\"label\"] == 0:\n",
    "        label_counts[\"0\"] += 1\n",
    "    elif item[\"label\"] == 1:\n",
    "        label_counts[\"1\"] += 1\n",
    "\n",
    "# Print the label counts\n",
    "print(\"Number of labels with value 0:\", label_counts[\"0\"])\n",
    "print(\"Number of labels with value 1:\", label_counts[\"1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfdbf78",
   "metadata": {},
   "source": [
    "### Webscrapping: The Guardian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7332b24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aria-labels saved successfully to the_guardian.json\n"
     ]
    }
   ],
   "source": [
    "def get_guardian_aria_labels(url):\n",
    "    # List of unwanted \"aria-label\" values\n",
    "    unwanted_labels = [\n",
    "        \"Toggle main menu\",\n",
    "        \"Toggle News\",\n",
    "        \"Toggle Opinion\",\n",
    "        \"Toggle Sport\",\n",
    "        \"Toggle Culture\",\n",
    "        \"Toggle Lifestyle\"\n",
    "    ]\n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all elements with \"aria-label\" attribute\n",
    "        elements_with_aria_label = soup.find_all(attrs={\"aria-label\": True})\n",
    "        \n",
    "        # Extract the content of \"aria-label\" attribute from each element\n",
    "        aria_labels = [\n",
    "            {\"label\": 0, \"text\": element[\"aria-label\"]}\n",
    "            for element in elements_with_aria_label\n",
    "            if element[\"aria-label\"] not in unwanted_labels\n",
    "        ]\n",
    "        \n",
    "        # Return the result as a JSON object\n",
    "        return json.dumps(aria_labels, indent=4)\n",
    "    else:\n",
    "        # If the request failed, return an error message as a JSON object\n",
    "        return json.dumps({\"error\": f\"Failed to retrieve the page. Status code: {response.status_code}\"}, indent=4)\n",
    "\n",
    "# URL of the Guardian homepage\n",
    "url = \"https://www.theguardian.com/international\"\n",
    "\n",
    "# Get the \"aria-label\" content as a JSON object\n",
    "aria_labels_json = get_guardian_aria_labels(url)\n",
    "\n",
    "# Save the result to a JSON file\n",
    "output_file = \"the_guardian.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(aria_labels_json)\n",
    "\n",
    "# Print a message indicating that the data has been saved\n",
    "print(f\"Aria-labels saved successfully to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "038bc0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects in the GUARDIAN JSON file: 114\n",
      "Number of labels with value 0: 114\n",
      "Number of labels with value 1: 0\n"
     ]
    }
   ],
   "source": [
    "# Open the JSON file and load the data\n",
    "with open(\"the_guardian.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Check the number of objects\n",
    "num_objects_guardian = len(data)\n",
    "\n",
    "print(\"Number of objects in the GUARDIAN JSON file:\", num_objects_guardian)\n",
    "\n",
    "# Open the JSON file and load the data\n",
    "with open(\"the_guardian.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Count the occurrences of labels\n",
    "label_counts = {\"0\": 0, \"1\": 0}\n",
    "for item in data:\n",
    "    if item[\"label\"] == 0:\n",
    "        label_counts[\"0\"] += 1\n",
    "    elif item[\"label\"] == 1:\n",
    "        label_counts[\"1\"] += 1\n",
    "\n",
    "# Print the label counts\n",
    "print(\"Number of labels with value 0:\", label_counts[\"0\"])\n",
    "print(\"Number of labels with value 1:\", label_counts[\"1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf486d",
   "metadata": {},
   "source": [
    "## Getting some negative news:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937eb37d",
   "metadata": {},
   "source": [
    "###  Webscrapping: Good News Agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "174e7dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content saved successfully to good_news.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_good_news(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all <strong> tags\n",
    "        strong_tags = soup.find_all('strong')\n",
    "        \n",
    "        # Extract text from the <strong> tags and format into dictionaries\n",
    "        data = [{\"label\": 1, \"text\": tag.get_text(strip=True)} for tag in strong_tags]\n",
    "        \n",
    "        # Return the result as a JSON object\n",
    "        return json.dumps(data, indent=4)\n",
    "    else:\n",
    "        # If the request failed, return an error message as a JSON object\n",
    "        return json.dumps({\"error\": f\"Failed to retrieve the page. Status code: {response.status_code}\"}, indent=4)\n",
    "\n",
    "# URL of the webpage\n",
    "url = \"http://www.goodnewsagency.org/m/issue.php?number=324&lang=en\"\n",
    "\n",
    "# Get the content as a JSON object\n",
    "content_json = scrape_good_news(url)\n",
    "\n",
    "# Save the result to a JSON file\n",
    "output_file = \"good_news.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(content_json)\n",
    "\n",
    "# Print a message indicating that the data has been saved\n",
    "print(f\"Content saved successfully to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e91bf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects in the GOOD NEWS JSON file: 111\n",
      "Number of labels with value 0: 0\n",
      "Number of labels with value 1: 111\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open the JSON file and load the data\n",
    "with open(\"good_news.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Check the number of objects\n",
    "num_objects_good_news = len(data)\n",
    "\n",
    "print(\"Number of objects in the GOOD NEWS JSON file:\", num_objects_good_news)\n",
    "\n",
    "# Open the JSON file and load the data\n",
    "with open(\"good_news.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Count the occurrences of labels\n",
    "label_counts = {\"0\": 0, \"1\": 0}\n",
    "for item in data:\n",
    "    if item[\"label\"] == 0:\n",
    "        label_counts[\"0\"] += 1\n",
    "    elif item[\"label\"] == 1:\n",
    "        label_counts[\"1\"] += 1\n",
    "\n",
    "# Print the label counts\n",
    "print(\"Number of labels with value 0:\", label_counts[\"0\"])\n",
    "print(\"Number of labels with value 1:\", label_counts[\"1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "788c5f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles extracted and saved to 'good_news_network_titles.json'.\n"
     ]
    }
   ],
   "source": [
    "def extract_titles(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all <a> tags with the specified attributes\n",
    "        a_tags = soup.find_all('a', attrs={'rel': 'bookmark'})\n",
    "        \n",
    "        # Extract titles from the title attribute of each <a> tag\n",
    "        titles = [{\"label\": 1, \"text\":tag.get('title', '')} for tag in a_tags]\n",
    "        \n",
    "        # Return the result as a JSON object\n",
    "        return json.dumps(titles, indent=4)\n",
    "    else:\n",
    "        # If the request failed, return an error message as a JSON object\n",
    "        return json.dumps({\"error\": f\"Failed to retrieve the page. Status code: {response.status_code}\"}, indent=4)\n",
    "\n",
    "# URL of the webpage\n",
    "url = \"https://www.goodnewsnetwork.org/category/news/world/\"\n",
    "\n",
    "# Get the titles as a JSON object\n",
    "titles_json = extract_titles(url)\n",
    "\n",
    "# Save the titles to a JSON file\n",
    "with open(\"good_news_network_titles.json\", \"w\") as file:\n",
    "    file.write(titles_json)\n",
    "\n",
    "# Print a success message\n",
    "print(\"Titles extracted and saved to 'good_news_network_titles.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "418e3c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects in the good_news_network_titles.json file: 34\n",
      "Number of labels with value 0: 0\n",
      "Number of labels with value 1: 34\n"
     ]
    }
   ],
   "source": [
    "# Open the JSON file and load the data\n",
    "with open(\"good_news_network_titles.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Check the number of objects\n",
    "num_objects_goodnews_network = len(data)\n",
    "\n",
    "print(\"Number of objects in the good_news_network_titles.json file:\", num_objects_goodnews_network)\n",
    "\n",
    "# Open the JSON file and load the data\n",
    "with open(\"good_news_network_titles.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Count the occurrences of labels\n",
    "label_counts = {\"0\": 0, \"1\": 0}\n",
    "for item in data:\n",
    "    if item[\"label\"] == 0:\n",
    "        label_counts[\"0\"] += 1\n",
    "    elif item[\"label\"] == 1:\n",
    "        label_counts[\"1\"] += 1\n",
    "\n",
    "# Print the label counts\n",
    "print(\"Number of labels with value 0:\", label_counts[\"0\"])\n",
    "print(\"Number of labels with value 1:\", label_counts[\"1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3694d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles extracted and saved to 'good_news_inspiring.json'.\n"
     ]
    }
   ],
   "source": [
    "def extract_titles(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all <a> tags with the specified attributes\n",
    "        a_tags = soup.find_all('a', attrs={'rel': 'bookmark'})\n",
    "        \n",
    "        # Extract titles from the title attribute of each <a> tag\n",
    "        titles = [{\"label\": 1, \"text\":tag.get('title', '')} for tag in a_tags]\n",
    "        \n",
    "        # Return the result as a JSON object\n",
    "        return json.dumps(titles, indent=4)\n",
    "    else:\n",
    "        # If the request failed, return an error message as a JSON object\n",
    "        return json.dumps({\"error\": f\"Failed to retrieve the page. Status code: {response.status_code}\"}, indent=4)\n",
    "\n",
    "# URL of the webpage\n",
    "url = \"https://www.goodnewsnetwork.org/category/news/inspiring/\"\n",
    "\n",
    "# Get the titles as a JSON object\n",
    "titles_json = extract_titles(url)\n",
    "\n",
    "# Save the titles to a JSON file\n",
    "with open(\"good_news_inspiring.json\", \"w\") as file:\n",
    "    file.write(titles_json)\n",
    "\n",
    "# Print a success message\n",
    "print(\"Titles extracted and saved to 'good_news_inspiring.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de10e755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects in the good_news_inspiring.json file: 35\n",
      "Number of labels with value 0: 0\n",
      "Number of labels with value 1: 35\n"
     ]
    }
   ],
   "source": [
    "# Open the JSON file and load the data\n",
    "with open(\"good_news_inspiring.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Check the number of objects\n",
    "num_objects_goodnews_inspiring = len(data)\n",
    "\n",
    "print(\"Number of objects in the good_news_inspiring.json file:\", num_objects_goodnews_inspiring)\n",
    "\n",
    "# Open the JSON file and load the data\n",
    "with open(\"good_news_inspiring.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Count the occurrences of labels\n",
    "label_counts = {\"0\": 0, \"1\": 0}\n",
    "for item in data:\n",
    "    if item[\"label\"] == 0:\n",
    "        label_counts[\"0\"] += 1\n",
    "    elif item[\"label\"] == 1:\n",
    "        label_counts[\"1\"] += 1\n",
    "\n",
    "    \n",
    "# Print the label counts\n",
    "print(\"Number of labels with value 0:\", label_counts[\"0\"])\n",
    "print(\"Number of labels with value 1:\", label_counts[\"1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df2d48f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total positive news:  193\n",
      "total negative news:  187\n"
     ]
    }
   ],
   "source": [
    "total_positive = num_objects_goodnews_network + num_objects_good_news + 13 + num_objects_goodnews_inspiring\n",
    "print(\"total positive news: \", total_positive)\n",
    "\n",
    "total_negative = num_objects_bbc + num_objects_guardian + 6\n",
    "print(\"total negative news: \", total_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c660b218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels with value 0: 120\n",
      "Number of labels with value 1: 120\n"
     ]
    }
   ],
   "source": [
    "# Open the JSON file and load the data\n",
    "with open(\"dataset.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# # Check the number of objects\n",
    "# num_objects_goodnews_network = len(data)\n",
    "\n",
    "# print(\"Number of objects in the good_news_network_titles.json file:\", num_objects_goodnews_network)\n",
    "\n",
    "# Open the JSON file and load the data\n",
    "with open(\"dataset.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Count the occurrences of labels\n",
    "label_counts = {\"0\": 0, \"1\": 0}\n",
    "for item in data:\n",
    "    if item[\"label\"] == 0:\n",
    "        label_counts[\"0\"] += 1\n",
    "    elif item[\"label\"] == 1:\n",
    "        label_counts[\"1\"] += 1\n",
    "\n",
    "# Print the label counts\n",
    "print(\"Number of labels with value 0:\", label_counts[\"0\"])\n",
    "print(\"Number of labels with value 1:\", label_counts[\"1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f0da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ae7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
